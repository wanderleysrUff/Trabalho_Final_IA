# Adversarial Debiasing para MitigaÃ§Ã£o de Vieses em Machine Learning

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Trabalho Final - Disciplina de InteligÃªncia Artificial  
Mestrado Profissional em Engenharia de ProduÃ§Ã£o e Sistemas Computacionais  
Universidade Federal Fluminense

## ğŸ“‹ Sobre o Projeto

Este projeto implementa tÃ©cnicas de **Adversarial Debiasing** para mitigaÃ§Ã£o de vieses em modelos de Machine Learning, utilizando o dataset **IBM HR Analytics Employee Attrition**. O objetivo Ã© criar modelos de prediÃ§Ã£o de rotatividade de funcionÃ¡rios que sejam justos em relaÃ§Ã£o a atributos sensÃ­veis como gÃªnero e idade.

### ğŸ¯ Objetivos

- Analisar vieses presentes em modelos de ML para prediÃ§Ã£o de attrition
- Implementar Adversarial Debiasing usando AIF360
- Realizar Grid Search de hiperparÃ¢metros (Î») para otimizaÃ§Ã£o
- Comparar mÃ©tricas de fairness antes e depois da mitigaÃ§Ã£o
- Avaliar trade-offs entre performance e equidade
- Utilizar SHAP para anÃ¡lise de explicabilidade

## ğŸ“Š Dataset

**IBM HR Analytics Employee Attrition Dataset**

- **Fonte:** [Kaggle](https://www.kaggle.com/datasets/pavansubhasht/ibm-hr-analytics-attrition-dataset)
- **Amostras:** 1.470 funcionÃ¡rios
- **Features:** 35 variÃ¡veis (demogrÃ¡ficas, profissionais, satisfaÃ§Ã£o)
- **Target:** Attrition (Yes/No)
- **Atributos sensÃ­veis:** Gender, Age

### CaracterÃ­sticas do Dataset

- **Desbalanceamento:** 84% No Attrition, 16% Yes Attrition
- **DistribuiÃ§Ã£o de gÃªnero:** ~60% Male, ~40% Female
- **Proxies identificados:** JobRole, Department (correlacionados com Gender)

## ğŸ—ï¸ Estrutura do Projeto

```
adversarial-debiasing-hr/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                           # Dataset original
â”‚   â”‚   â””â”€â”€ WA_Fn-UseC_-HR-Employee-Attrition.csv
â”‚   â””â”€â”€ processed/                     # Dados processados
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ adversarial_debiasing_complete.ipynb  # Notebook principal
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ save_results.py                # Salvamento de resultados
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ figures/                       # GrÃ¡ficos e visualizaÃ§Ãµes (300 DPI)
â”‚   â”œâ”€â”€ metrics/                       # MÃ©tricas salvas (CSV/JSON)
â”‚   â””â”€â”€ models/                        # Modelos treinados
â”œâ”€â”€ requirements.txt                   # DependÃªncias
â”œâ”€â”€ .gitignore                         # Arquivos ignorados pelo Git
â”œâ”€â”€ README.md                          # Este arquivo
â””â”€â”€ LICENSE                            # LicenÃ§a do projeto
```

## ğŸ”§ InstalaÃ§Ã£o e ConfiguraÃ§Ã£o

### PrÃ©-requisitos

- Python 3.8 ou superior
- pip (gerenciador de pacotes Python)
- Git

### InstalaÃ§Ã£o

1. **Clone o repositÃ³rio:**

```bash
git clone https://github.com/seu-usuario/adversarial-debiasing-hr.git
cd adversarial-debiasing-hr
```

2. **Crie um ambiente virtual (recomendado):**

```bash
# Windows
python -m venv venv
venv\Scripts\activate

# Linux/Mac
python3 -m venv venv
source venv/bin/activate
```

3. **Instale as dependÃªncias:**

```bash
pip install -r requirements.txt
```

4. **Baixe o dataset:**

OpÃ§Ã£o 1 - Manual:

- Acesse [Kaggle Dataset](https://www.kaggle.com/datasets/pavansubhasht/ibm-hr-analytics-attrition-dataset)
- Baixe o arquivo CSV
- Coloque em `data/raw/`

## ğŸš€ Como Executar

### OpÃ§Ã£o 1: Jupyter Notebook Local

```bash
# Ativar ambiente virtual
venv\Scripts\activate     # Windows
source venv/bin/activate  # Linux/Mac

# Iniciar Jupyter
jupyter notebook

# Abrir notebooks/adversarial_debiasing_complete.ipynb
```

### OpÃ§Ã£o 2: VSCode

1. Abra a pasta do projeto no VSCode
2. Instale a extensÃ£o "Jupyter"
3. Abra `notebooks/adversarial_debiasing_complete.ipynb`
4. Selecione o kernel do ambiente virtual
5. Execute as cÃ©lulas sequencialmente

### OpÃ§Ã£o 3: Python EspecÃ­fico (Windows)

```bash
py -3.11 -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
jupyter notebook
```

## ğŸ“ˆ Metodologia

### 1. AnÃ¡lise ExploratÃ³ria

- AnÃ¡lise de distribuiÃ§Ãµes (target, variÃ¡veis sensÃ­veis)
- IdentificaÃ§Ã£o de correlaÃ§Ãµes e proxies
- Testes estatÃ­sticos (Chi-quadrado)
- VisualizaÃ§Ãµes de viÃ©s por grupo protegido

### 2. Preprocessamento

- Encoding de variÃ¡veis categÃ³ricas (LabelEncoder)
- NormalizaÃ§Ã£o/padronizaÃ§Ã£o (StandardScaler)
- Split treino/teste (70/30) estratificado
- Tratamento de desbalanceamento com **SMOTE** (k_neighbors=5)

### 3. Modelos Baseline

Foram testados **dois modelos baseline** para comparaÃ§Ã£o:

- **Baseline v1:** Random Forest com `class_weight='balanced'`
- **Baseline v2:** Random Forest treinado com dados balanceados por SMOTE

**ConfiguraÃ§Ã£o:**

- n_estimators: 100
- max_depth: 10
- random_state: 42

**CritÃ©rio de seleÃ§Ã£o:** Modelo com maior F1-Score

### 4. Adversarial Debiasing

**Grid Search de HiperparÃ¢metros:**

ImplementaÃ§Ã£o usando AIF360 com otimizaÃ§Ã£o do parÃ¢metro Î» (adversary_loss_weight):

- **Lambda testados:** [0.01, 0.05, 0.1, 0.2, 0.5]
- **ConfiguraÃ§Ã£o:**
  - Ã‰pocas: 50
  - Batch size: 128
  - Atributo sensÃ­vel: Gender (Female = 1, Male = 0)

**CritÃ©rio de seleÃ§Ã£o:**

1. Filtrar modelos com Demographic Parity < 0.1
2. Entre os vÃ¡lidos, selecionar o com maior F1-Score

**Resultado:** Î» Ã³timo varia entre 0.2 e 0.5 dependendo da execuÃ§Ã£o (devido Ã  natureza estocÃ¡stica do treinamento)

### 5. AnÃ¡lise de Explicabilidade

- **SHAP values** para interpretaÃ§Ã£o de features
- IdentificaÃ§Ã£o das top 10 features mais importantes
- AnÃ¡lise de proxies e suas relaÃ§Ãµes com atributos sensÃ­veis
- Summary plots e feature importance

## ğŸ“Š Resultados

### Performance dos Modelos (Exemplo - Resultados variam por execuÃ§Ã£o)

| Modelo                     | Accuracy | F1-Score | AUC-ROC |
| -------------------------- | -------- | -------- | ------- |
| Baseline v1 (class_weight) | 0.8614   | 0.8500   | 0.8300  |
| Baseline v2 (SMOTE)        | 0.8367   | 0.8402   | 0.8100  |
| Adversarial (Î»=0.2)        | 0.8617   | 0.8621   | -       |
| Adversarial (Î»=0.5)        | 0.8503   | 0.8520   | -       |

### MÃ©tricas de Fairness (Female vs Male)

| MÃ©trica                 | Baseline v2 | Adversarial (Î»=0.5) | Melhoria       |
| ----------------------- | ----------- | ------------------- | -------------- |
| Demographic Parity Diff | 0.1139      | 0.0121              | âœ… 89% melhor  |
| Disparate Impact        | 0.5444      | 1.0730              | âœ… Quase ideal |
| Equal Opportunity Diff  | 0.0214      | 0.0821              | -              |

### Grid Search de Lambda

O experimento demonstrou que:

- **Î» baixo (0.01-0.05):** Prioriza accuracy, fairness moderada
- **Î» mÃ©dio (0.1-0.2):** Bom equilÃ­brio, pode ter melhor accuracy
- **Î» alto (0.5):** Excelente fairness, mantÃ©m alta performance

**ObservaÃ§Ã£o importante:** Devido Ã  natureza estocÃ¡stica das redes neurais adversariais, diferentes execuÃ§Ãµes podem selecionar Î»=0.2 ou Î»=0.5 como Ã³timo. Ambos apresentam excelentes resultados.

### Principais Insights

1. âœ… **Adversarial Debiasing reduziu significativamente o viÃ©s**

   - Demographic Parity: ~0.11 â†’ ~0.01-0.05 (reduÃ§Ã£o de atÃ© 90%)
   - Disparate Impact prÃ³ximo de 1.0 (ideal)

2. âš–ï¸ **Trade-off aceitÃ¡vel ou inexistente:**

   - Em alguns casos, Î»=0.5 manteve accuracy similar ao baseline
   - Em outros, Î»=0.2 atÃ© SUPEROU o baseline em performance
   - Perda mÃ¡xima de accuracy < 3% quando ocorre

3. ğŸ” **Proxies identificados:**

   - JobRole e Department correlacionam fortemente com Gender
   - Modelo adversarial aprende a ignorar esses atalhos

4. ğŸ“Š **SMOTE vs class_weight:**

   - Ambas as abordagens sÃ£o vÃ¡lidas
   - Baseline v2 (SMOTE) geralmente selecionado por melhor F1-Score

5. ğŸ² **Variabilidade estocÃ¡stica:**
   - Resultados variam ligeiramente entre execuÃ§Ãµes
   - Zona Ã³tima de Î» entre 0.2-0.5 consistentemente identificada
   - Demonstra robustez da metodologia

## ğŸ› ï¸ Tecnologias Utilizadas

- **Python 3.8+**
- **Bibliotecas principais:**
  - `pandas 2.0+`, `numpy 1.24+` - ManipulaÃ§Ã£o de dados
  - `scikit-learn 1.3+` - Modelos de ML e preprocessamento
  - `aif360 0.5+` - Fairness e Adversarial Debiasing
  - `fairlearn 0.9+` - MÃ©tricas de fairness adicionais
  - `shap 0.42+` - Explicabilidade (SHAP values)
  - `imbalanced-learn 0.11+` - SMOTE para balanceamento
  - `tensorflow 2.x` - Backend para Adversarial Debiasing
  - `matplotlib 3.7+`, `seaborn 0.12+` - VisualizaÃ§Ãµes
  - `jupyter` - Ambiente interativo

## ğŸ“ ConfiguraÃ§Ãµes do Experimento

Todas as configuraÃ§Ãµes podem ser ajustadas no dicionÃ¡rio `CONFIG`:

```python
CONFIG = {
    'test_size': 0.3,               # 70/30 split
    'smote_k_neighbors': 5,         # Vizinhos para SMOTE
    'rf_n_estimators': 100,         # Ãrvores no Random Forest
    'rf_max_depth': 10,             # Profundidade mÃ¡xima
    'adversarial_epochs': 50,       # Ã‰pocas de treinamento
    'adversarial_batch_size': 128,  # Tamanho do batch
    'adversarial_lambda': 0.1,      # Peso inicial (ajustado por grid search)
    'shap_sample_size': 500         # Amostras para SHAP
}
```

## ğŸ“š ReferÃªncias

1. **Zhang, B. H., Lemoine, B., & Mitchell, M. (2018).** _Mitigating Unwanted Biases with Adversarial Learning._ AIES 2018. [arXiv:1801.07593](https://arxiv.org/abs/1801.07593)

2. **Bellamy, R. K. E., et al. (2019).** _AI Fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias._ IBM Journal of Research and Development, 63(4/5).

3. **Lundberg, S. M., & Lee, S. I. (2017).** _A Unified Approach to Interpreting Model Predictions._ NeurIPS 2017.

4. **Hardt, M., Price, E., & Srebro, N. (2016).** _Equality of Opportunity in Supervised Learning._ NeurIPS 2016.

5. **Chawla, N. V., et al. (2002).** _SMOTE: Synthetic Minority Over-sampling Technique._ Journal of Artificial Intelligence Research, 16, 321-357.

### Links Ãšteis

- [AIF360 Documentation](https://aif360.readthedocs.io/)
- [Fairlearn](https://fairlearn.org/)
- [SHAP Documentation](https://shap.readthedocs.io/)
- [IBM HR Analytics Dataset](https://www.kaggle.com/datasets/pavansubhasht/ibm-hr-analytics-attrition-dataset)
- [Imbalanced-Learn](https://imbalanced-learn.org/)

## ğŸ”¬ Reprodutibilidade

Para garantir reprodutibilidade dos resultados:

1. **Seeds fixadas:** `RANDOM_STATE = 42` em todos os componentes aleatÃ³rios
2. **Eager execution desabilitada:** NecessÃ¡rio para AIF360
3. **VersÃµes fixas:** Use o `requirements.txt` fornecido

**Nota:** Devido Ã  natureza estocÃ¡stica do Adversarial Debiasing (redes neurais), pequenas variaÃ§Ãµes nos resultados sÃ£o esperadas entre execuÃ§Ãµes. O grid search foi implementado justamente para identificar a zona robusta de hiperparÃ¢metros.

## ğŸ’¾ Resultados Salvos

Ao executar o notebook, os seguintes arquivos sÃ£o gerados automaticamente em `results/`:

### `figures/` (300 DPI)

- DistribuiÃ§Ã£o de Attrition
- AnÃ¡lise de variÃ¡veis sensÃ­veis (Gender, Age)
- ComparaÃ§Ã£o SMOTE (antes/depois)
- Confusion matrices (Baseline e Adversarial)
- SHAP summary plots
- Grid search visualizations (trade-off plots)

### `metrics/`

- `comparison_performance.csv` - ComparaÃ§Ã£o de performance
- `comparison_fairness.csv` - ComparaÃ§Ã£o de fairness
- `feature_importance.csv` - ImportÃ¢ncia das features (SHAP)
- `grid_search_lambda.csv` - Resultados de todos os lambdas testados

### `models/`

- `baseline_model.pkl` - Modelo baseline selecionado
- `scaler.pkl` - StandardScaler treinado
- `label_encoders.pkl` - Encoders das variÃ¡veis categÃ³ricas

## ğŸ‘¥ Autores

**Christian Ferreira**
**Wanderley Rangel**

**Orientador:** Prof. Dr. Leonard Barreto Moreira  
Universidade Federal Fluminense - UFF

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT - veja o arquivo [LICENSE](LICENSE) para detalhes.

---
